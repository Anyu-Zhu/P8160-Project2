---
title: "coordinate_descent_alg"
author: "Renjie Wei"
date: '2022-03-24'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r CD_lasso_logistic}
soft_threshold <- function(beta, lambda) {
    sign(beta) * max(abs(beta) - lambda, 0)
}

######## logistic version ########

# calculate z, px, w each turn, then update betaj

getP <- function(X, betavec){
    Px <- 1/(1 + exp(-(X %*% betavec)))
    return(Px)
}

getW <- function(Px){
    W <- Px*(1-Px)
    return(Px)
}

getZ <- function(X, y, betavec, Px, W){
    Z <- X %*% betavec + (y - Px)/W
    return(Z)
}

lassoCD <- function(
        X, y, lambda = 0.5, init_beta = NULL, max_iter = 500, tol = 1e-8
){
    betavec <- init_beta
    N <- length(y)
    i <- 0
    loss <- 1e5
    prevloss <- Inf
    res <- c(0, loss, betavec)
    cont <- TRUE
    while(i <= max_iter && cont){
        i <- i + 1
        prevloss <- loss
        for(j in 1:length(betavec)){
            #if(betavec[j] == 0){
            #    next
            #}
            Px <- getP(X, betavec)
            Px <- ifelse(abs(1 - Px) < 1e-5, 1, ifelse(abs(Px) < 1e-5, 0, Px))
            W <- ifelse(abs(getW(Px)) < 1e-5, 1e-5, getW(Px))
            Z <- getZ(X, y, betavec, Px, W)
            betaresj <- betavec
            betaresj[j] <- 0
            #Presj <- getP(X, betaresj)
            #Presj <- ifelse(abs(1 - Presj) < 1e-5, 1, ifelse(abs(Presj) < 1e-5, 0, Presj))
            #Wresj <- ifelse(abs(getW(Presj)) < 1e-5, 1e-5, getW(Presj))
            #Zresj <- getZ(X, y, betaresj, Presj, Wresj)
            Zresj <- X %*% betaresj
            betaj <- soft_threshold(mean(W * X[,j] * (Z - Zresj)), lambda)/sum(W * X[,j] * X[,j])
            betavec[j] <- betaj
            loss <- (1/(2*N))*sum(W * (Z - Zresj)^2) + lambda * sum(abs(betavec))
        }
        print(loss)
        if(abs(prevloss - loss) < tol || loss > Inf){
            cont <- FALSE
        }
        res <- rbind(res, c(i, loss, betavec))
    }
    return(res)
}

```
\newpage
```{r eval=FALSE, tidy=FALSE}
# define soft_threshold, getP, getW, getZ

lassoCD <- function(
        X, y, lambda = 0.5, init_beta = NULL, max_iter = 500, tol = 1e-8
){
    betavec <- init_beta
    N <- length(y)
    i <- 0
    loss <- 1e5
    prevloss <- Inf
    res <- c(0, loss, betavec)
    cont <- TRUE
    while(i <= max_iter && cont){
        i <- i + 1
        prevloss <- loss
        for(j in 1:length(betavec)){
            Z <- getZ(X, y, betavec, Px, W)
            # setting beta_j to 0
            betaresj <- betavec
            betaresj[j] <- 0
            Zresj <- getZ(X, y, betaresj, Presj, Wresj)
            # update beta_j by solving penalized weighted least-square
            betaj <- soft_threshold(mean(W * X[,j] * (Z - Zresj)), lambda)/sum(W * X[,j] * X[,j])
            betavec[j] <- betaj
            loss <- (1/(2*N))*sum(W * (Z - Zresj)^2) + lambda * sum(abs(betavec))
        }
        # convergency
        if(abs(prevloss - loss) < tol || loss > Inf){
            cont <- FALSE
        }
        res <- rbind(res, c(i, loss, betavec))
    }
    return(res)
}
```



```{r CD_lasso_mtx}
## matrix version ##
#lassoCD.matrix <- function(
#        X, y, lambda = 0.5, init_beta = NULL, max_iter = 1e4, tol = 1e-8
#){
#    betavec <- init_beta
#    N <- length(y)
#    i <- 0
#    loss <- 1e5
#    prevloss <- Inf
#    res <- c(0, loss, betavec)
#    cont <- TRUE
#    while(i <= max_iter && cont){
#        i <- i + 1
#        prevloss <- loss
#        for(j in 1:length(betavec)){
#            Px <- getP(X, betavec)
#            W <- getW(Px)
#            #W <- ifelse(abs(W-0) < 1e-5, 1e-5, W)
#            Wdiag <- diag(c(W), N, N)
#            Z <- getZ(X, y, betavec, Px, W)
#            betaresj <- betavec
#            betaresj[j] <- 0
#            Zresj <- X %*% betaresj
#            betaj <- 
#                soft_threshold((1/N) * (t(X[,j]) %*% Wdiag %*% (Z-(X %*% betaresj))),
#                               lambda)/(t(X[,j]) %*% Wdiag %*% X[,j])
#            betavec[j] <- betaj
#            loss <- (1/(2*N))*sum(Wdiag %*% (Z - X %*% betavec)^2) + lambda * sum(abs(betavec))
#        }
#        print(loss)
#        if(abs(prevloss - loss) < tol || loss > Inf){
#            cont <- FALSE
#        }
#        res <- rbind(res, c(i, loss, betavec))
#    }
#    return(res)
#}
```

```{r for_CD_lasso_pre}
lassoCD.predict <- function(betavec, X_new, y){
    Py <- 1/(1 + exp(-(X_new %*% betavec)))
    Pn <- 1-Py
    res <- list(P_pos = Py, P_neg = Pn, res = Py > Pn)
    res$res = as.numeric(res$res)
    return(res)
}

```

```{r test_alg}
library(tidyverse)
cancer = read.csv("breast-cancer.csv") %>% 
    mutate(diagnosis = factor(diagnosis))

mean(cancer$radius_mean)
sqrt(var(cancer$radius_mean))

cancer$diagnosis = case_when(cancer$diagnosis == "M" ~ 1,
                             TRUE ~ 0)
data.cancer = data.frame(cancer[3:32])
data.cancer["intercept"] = 1
data.cancer = as.matrix(data.cancer)

cancer.lassoCD <- lassoCD(data.cancer, cancer$diagnosis, init_beta = c(rep(0,31)), lambda = 0.01)
cancer.lassoCD.predict <- lassoCD.predict(cancer.lassoCD[2,3:33], data.cancer, cancer$diagnosis)

pred.obs <- data.frame(pred = cancer.lassoCD.predict$res, obs = cancer$diagnosis)
pred.obs$mis = pred.obs$pred == pred.obs$obs
sum(pred.obs$mis)

library(glmnet)
glmnet.cancer = glmnet(data.cancer, cancer$diagnosis, family = "binomial", lambda = 0, intercept = FALSE)
predict.glmnet(glmnet.cancer, s = 0.5, type = "coefficient")


library(pROC)
roc(pred.obs$pred, pred.obs$obs)
```

```{r}
path <- function(X, y, lambdaseq){
    init_beta <- rep(0, dim(X)[2])
    betas <- NULL
    for (i in 1:length(lambdaseq)) {
        cd.result <- lassoCD(X, y, lambda = lambdaseq[i], init_beta = init_beta)
        last_beta <- cd.result[nrow(cd.result),3:dim(cd.result)[2]]
        init_beta <- last_beta
        betas <- rbind(betas,c(last_beta))
        i <- i + 1
    }
    return(data.frame(cbind(lambdaseq,betas)))
}
path.out <- path(data.cancer, cancer$diagnosis,lambdaseq = exp(seq(1,-10, length=100)))
colnames(path.out) <- c("lambda",colnames(cancer)[3:32],"intercept")
# plot a path of solutions
path.plot <- path.out %>%
  gather(key = par, value = estimate, c(2:dim(path.out)[2])) %>% 
  ggplot(aes(x = log(lambda), y = estimate, group = par, col = par)) +
  geom_line()+
  ggtitle("A path of solutions with a sequence of descending lambda's") +
  xlab("log(Lambda)") + 
  ylab("Estimate") +
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 6))
path.plot

```


```{r}
cvresult <- function(X, y, grid, K){
  n <- dim(X)[1]
  folds <- sample(1:K, n, replace=TRUE)
  start <- rep(0, dim(X)[2])
  cv.error <- vector()
  cv.se <- vector()
  for (x in 1:length(grid)) {
  cv.errors <- vector()
  for (i in 1:K){
    cor.result <- coordinatelasso(lambda = grid[x], 
                                  dat = list(X=as.matrix(inputx[folds!=i,]),y=inputy[folds!=i]),
                                  s = start)
    lasbeta <- cor.result[nrow(cor.result),3:dim(cor.result)[2]]
    u <- as.matrix(inputx[folds == i,]) %*% lasbeta
    expu <- exp(u) 
    prob <- expu / (1 + expu) 
    y <- as.vector(inputy[folds==i])
    cv.errors[i] = mean((y-prob)^2) #MSE
    start <- lasbeta
  }
  cv.error[x] <- mean(cv.errors)
  cv.se[x] <- sqrt(var(cv.errors)/K)
  }
  return(cbind(grid,cv.error,cv.se))
}
```

```{r}
#library(glmnet)
#library(caret)
#
#ctrl <- trainControl(method = "cv",
#                     summaryFunction = twoClassSummary,
#                     classProbs = TRUE)
#glmnGrid <- expand.grid(alpha = 1, lambda = exp(seq(2,-10, length = 100)))
#model.glmn <- train(x = scale(data.cancer),
#                   y = as.factor(ifelse(cancer$diagnosis == 1, "pos", "neg")),
#                   method = "glmnet",
#                   tuneGrid = glmnGrid,
#                   metric = "ROC",
#                   trControl = ctrl)
#model.glmn$bestTune
#glmn.coef = coef(model.glmn$finalModel)
#
#model.glmnet = glmnet(scale(data.cancer), cancer$diagnosis, family = "binomial", lambda = 0.280, intercept = #FALSE)
#predict.glmnet(model.glmnet , s = 0.280, type = "coefficient")
```


Regularization is the common variable selection approaches for high-dimensional covariates. The best known Regularization is called LASSO, which is to add $L1$-penalty to the objective function. First we consider a linear regression, with penalty term, we are aiming to solve the following problem: 
$$\min_{\beta \in \mathbb{R}^{p+1}} \{ \frac{1}{2n} \sum_{i=1}^{n}(z_{i} - \sum_{j=0}^{p}x_{i,j}\beta_{j})^{2} + \lambda P{(\beta)})\}$$
When $p$, the number of predictors is large, the optimization could be challenging, so we could use coordinate-wise descent update with objective function
$$
f(\beta_{j}) = \frac{1}{2n}\sum_{i=1}^{n}(y_{i} - \sum_{k \neq j}x_{i,k}\tilde{\beta_{k}} - x_{i,j}\beta_{j})^{2} + \lambda\sum_{k \neq j}|\tilde{\beta_{k}}| + \lambda|\beta_{j}|
$$
Minimize $f(\beta_{j})$ w.r.t. $\beta_{j}$ while having $\tilde{\beta_{k}}$ fixed, we have
$$
\tilde{\beta}^{lasso}_{j}(\lambda) \leftarrow S(\frac{1}{n}\sum_{i=1}^{n}x_{i,j}(y_{i} - \tilde{y_{i}}^{(-j)}), \lambda)
$$
where $\tilde{y_{i}}^{(-j)} = \sum_{k \neq j}x_{i,k}\tilde{\beta_{k}}$ and $S(\hat{\beta}, \lambda) = sign(\hat{\beta})(|\hat{\beta}| - \lambda)_{+}$.

If a weight $\omega_{i}$ is then associated with each observation. In this case the update becomes only slightly more complicated:
$$
\tilde{\beta}^{lasso}_{j}(\lambda) \leftarrow \frac{S(\sum_{i=1}^{n}\omega_{i}x_{i,j}(y_{i} - \tilde{y_{i}}^{(-j)}), \lambda)}{\sum_{i=1}^n\omega_{i}x_{i,j}^{2}}
$$

In the context of logistic regression, we are aiming to maximize the penalized log likelihood:
$$\max_{\beta \in \mathbb{R}^{p+1}} \frac{1}{n}\sum_{i=1}^n  \{y_i(X_{i}\beta)-\log(1+\exp(X_{i}\beta))\}-\lambda \sum_{j=0}^p |\beta_j|$$
for some $\lambda \geq 0$. Here the $x_{i,j}$ are standardized so that $\sum_{i}x_{i,j}/n = 0$ and $\sum_{i}x_{i,j}^{2}/n = 0$.

The Newton algorithm for maximizing the log likelihood amounts to interatively reweighted least squares. Hence if the current estimate of the parameter is $\tilde{\beta}$, we can form a quardratic approximation to the negative log likelihood by taylor expansion around the current estimate, which is:
$$
f(\beta) = -\frac{1}{2n}\sum_{i=1}^{n}w_i(z_{i} - \sum_{j=0}^{p}x_{i,j}\beta_{j})^{2} + C(\tilde{\beta})
$$
where 
\[
\begin{aligned}
& z_i = \tilde{\beta}_0+ x_i^T\tilde{\beta} + \frac{y_i-\tilde{p}(x_i)}{\tilde{p}(x_i)(1-\tilde{p}(x_i))}, \text{working response}\\
& w_i = \tilde{p}(x_i)(1-\tilde{p}(x_i)), \text{working weights}
\end{aligned}
\]
and $\tilde{p}(x_i)$ is evaluated at the current parameters, the last term is constant. The Newton update is obtained by minimizing the $f(\beta)$

The coordinate descent algorithm to solve the penalized weighted least squares problem 
$$\min_{\beta \in \mathbb{R}^{p+1}} \{ -f(\beta) + \lambda P(\beta)\}$$
And the lasso update is :

$$
\tilde{\beta}_j^{\text{lasso}} \leftarrow \frac{S(\frac{1}{n}\sum_{i=1}^{n}w_ix_{ij}(z_i - \tilde z_i^{(j)}),\lambda)}{\sum_{i=1}^{n}w_ix_{ij}^2}
$$

The above amounts to a sequence of nested loops:

- outer loop: start with $\lambda$ that all the coefficients are forced to be zerp, then decrement $\lambda$;
- middle loop: update the quardratic $f(\beta)$ using the current estimates of parameters;
- inner loop: run the coordinate descent algorithm on the penalized weighted least square problem.

In our problem, care is taken to avoid coefficients diverging in order to achieve fitted probabilities of 0 or 1 which is the warning message by the R package. When a probability with $\epsilon = 1e-5$ of 1, we set it to 1, and set the weights to $\epsilon$. 0 is treated similarly.

