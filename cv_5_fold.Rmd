---
title: "5-fold cv"
author: "Yijing Tao"
date: "3/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(glmnet)
library(pdp)
library(caret)
library(modelr)
```

## data preprocessing

```{r data, message = FALSE }
standardize = function(col) {
  mean = mean(col)
  stdev = sd(col)
  return((col - mean)/stdev)
}

# just standardize the covariates
standardized.data = read.csv(file = "breast-cancer.csv") %>% 
  dplyr::select(radius_mean:fractal_dimension_worst) %>% 
  map_df(.x = ., standardize)

# add back in the response and ids
data = cbind(read.csv(file = "breast-cancer.csv") %>% dplyr::select(diagnosis), standardized.data) %>% 
  mutate(diagnosis = ifelse(diagnosis == "M", 1, 0))

cancer = read.csv("breast-cancer.csv") %>% 
  janitor::clean_names() %>% 
  mutate(diagnosis = as.numeric(factor(diagnosis)) - 1) %>% 
  select(-id)
y = as.matrix(cancer$diagnosis)
x = scale(as.matrix(cancer[, 2:31]))
```

## lassocd
```{r}
soft_threshold <- function(beta, lambda) {
    sign(beta) * max(abs(beta) - lambda, 0)
}

getP <- function(X, betavec){
    Px <- 1/(1 + exp(-(X %*% betavec)))
    return(Px)
}

getW <- function(Px){
    W <- Px*(1-Px)
    return(Px)
}

getZ <- function(X, y, betavec, Px, W){
    Z <- X %*% betavec + (y - Px)/W
    return(Z)
}


lassoCD <- function(
        X, y, lambda, init_beta, max_iter = 1e4, tol = 1e-8
){
    betavec <- init_beta
    N <- length(y)
    i <- 0
    loss <- 1e5
    prevloss <- Inf
    res <- c(0, loss, betavec)
    cont <- TRUE
    while(i <= max_iter && cont){
        i <- i + 1
        prevloss <- loss
        for(j in 1:length(betavec)){
            Px <- getP(X, betavec)
            W <- getW(Px)
            W <- ifelse(abs(W-0) < 1e-5, 1e-5, W)
            Z <- getZ(X, y, betavec, Px, W)
            betaresj <- betavec
            betaresj[j] <- 0
            Zresj <- X %*% betaresj
            betaj <- 
                soft_threshold(mean(W * X[,j] * (Z - Zresj)), lambda)/mean(W * X[,j] * X[,j])
            betavec[j] <- betaj
            loss <- (1/(2*N))*sum(W * (Z - X %*% betavec)^2) + lambda * sum(abs(betavec))
        }
        #print(loss)
        if(abs(prevloss - loss) < tol || loss < Inf){
            cont <- FALSE
        }
        res <- rbind(res, c(i, loss, betavec))
    }
    return(res)
}

# x1 = data2[1:50, 3:32]
# y1 = data2[1:50, 2]
# lassoCD(as.matrix(x1), y1, 0.5, c(rep(3, 30)), max_iter = 1e4, tol = 1e-8)

coordinatelasso <- function(lambda, dat, s, tol=1e-10, maxiter = 200){
    i <- 0 
    pp <- length(s)
    n <- length(dat$y)
    betavec <- s
    loglik <- 1e6
    res <- c(0, loglik, betavec)
    prevloglik <- Inf # To make sure it iterates 
    while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < inf) {
        i <- i + 1 
        prevloglik <- loglik
        for (j in 1:pp) {
            u <- dat$X %*% betavec
            expu <- exp(u) 
            prob <- expu/(expu+1)
            w <- prob*(1-prob) # weighted
            # avoid coeffcients diverging in order to achieve fitted  probabilities of 0 or 1.
            w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
            z <- u + (dat$y-prob)/w
            # calculate noj
            znoj <- dat$X[,-j] %*% betavec[-j]
            # revise the formula to be z
            betavec[j] <- soft_threshold(mean(w*(dat$X[,j])*(z - znoj)), lambda)/(mean(w*dat$X[,j]*dat$X[,j]))
        }
        loglik <- sum(w*(z-dat$X %*% betavec)^2)/(2*n) + lambda * sum(abs(betavec))
        res <- rbind(res, c(i, loglik, betavec))}  
    return(res)
}
```

```{r}
lambda.cv = function(lambdas, x, y, k) {
  data = as.data.frame(cbind(x, y))
  folds = crossv_kfold(data, k = k)
  
  start = rep(0, 31)
  fold.mse <- vector()
  fold.se <- vector()

  for (j in 1:length(lambdas)) {
    fold.errors <- vector()
    for (i in 1:k) {
      trainrow= folds[i,1][[1]][[toString(i)]]$idx
      testrow = folds[i,2][[1]][[toString(i)]]$idx
      
      train.X = x[trainrow,] 
      train.X = cbind(rep(1,dim(train.X)[1]),train.X)
      test.X = x[testrow,] 
      test.X = cbind(rep(1,dim(test.X)[1]),test.X)
      
      train.y = y[trainrow,] 
      test.y = y[testrow,] 
      
      # Perform the logistic-LASSO
      fit = lassoCD(train.X, train.y, lambda = lambdas[j], init_beta = start)
      betas = fit[nrow(fit),2:32]
      u = test.X %*% betas
      expu = exp(u)
      prob = expu / (1 + expu)
      # Calculate the test MSE for the fold
      fold.errors[i] = mean((test.y - prob)^2)
    }
    start = betas
    fold.mse[j] = mean(fold.errors)
    fold.se[j] = sqrt(var(fold.errors)/k)
  }
  return(cbind(log.lambda = log(lambdas), fold.mse, fold.se))
}

lambda.seq = exp(seq(-1,-10, length=100))

cv.path = lambda.cv(lambda.seq, x, y, 5)
cv.path = as.data.frame(cv.path)
min.mse = min(cv.path$fold.mse)
min.lambda = cv.path[which(cv.path$fold.mse == min.mse),]$log.lambda

cv.path %>%
  ggplot(data = ., aes(x = log.lambda, y = fold.mse)) +
  geom_vline(xintercept = min.lambda) +
  geom_line(color = "red") +
  geom_errorbar(aes(ymin = fold.mse - fold.se, ymax = fold.mse + 
                      fold.se), color = "gray50") +
  labs(
   title = "Average Test Fold MSE as a function of lambda",
   x = "log(lambda)",
   y = "Average Test MSE"
   ) +
 theme(plot.title = element_text(hjust = 0.5))
```

## 5-Fold Cross Validation

In order to find the optimal lambda, we use 5-fold cross validation. The dataset is divided into five subdatasets by using the *crossv_kfold* function. We combined 4 of them as the training data set, and the rest $\frac{1}{5}$ of them as the test data set. The optimal coefficients is then found five times by running the logit-LASSO on the training data set, leaving a different subset out each time. The subset left out is then used to estimate the model performance. This is done for all lambdas in the pre-defined sequence in order to search for the lambda with the highest average predictive ability. 
We use RMSE as the criteria to choose the best tuning parameter and corresponding model obtained from train data, and calculating RMSE to evaluate the model performance in test dataset.
$$RMSE = \sqrt{(test.y - predict.y)^2}$$

As a result, we got the best $\lambda$ is about 0 after the 5-fold cross validation.