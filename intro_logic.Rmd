---
title: "Intro_logistic"
author: "Anyu Zhu"
date: "3/24/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(caret)
library(corrplot)
```

```{r}
cancer = read.csv("breast-cancer.csv") %>% 
  mutate(diagnosis = factor(diagnosis))

n_obs = nrow(cancer)
n_var = ncol(cancer)
n_m = sum(cancer$diagnosis == "M")
n_b = n_obs - n_m
```


## Introduction

### Background and Objective
Breast cancer mainly occurs in middle-aged and older women. The median age at the time of breast cancer diagnosis is 62. This means half of the women who developed breast cancer are 62 years of age or younger when they are diagnosed. The goal of the project is to build a predictive model based on logistic regression to facilitate cancer diagnosis. We first build a logistic model to classify the images, then developed a Newton-Raphson algorithm to estimate the parameters of the logistic model. Then, we built a logistic-LASSO model to select features. Finally, we applied 5-fold cross-validation to select the best $\lambda$ for the logistic-LASSO model. 

### Data Preprocessing
The dataset 'breast-cancer'we used contains 569 rows and 32 columns. The variable `diagnosis` identifies if the image is coming from cancer tissue or benign. We labeled `malignant` as 1 and `benign` as 0. In total there are 212 malignant cases and 357 benign cases. There are 30 variables corresponding to mean, standard deviation and the largest values (points on the tails) of the distributions of 10 features: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.

Figure q displays the correlation between variables. We can see the correlation coefficient is large between several pairs of variables, which will potentially cause the problem of not converging in Newton-Raphson algorithm and Logistic Lasso model.  

```{r}
cancer_numeric = cancer[, 3:32]
corrplot(cor(cancer_numeric), method = 'color', tl.col = "blue", tl.cex = 0.5, 
         tl.srt = 45, title = "Figure 1: Correlation plot of all variables", mar=c(0,0,1,0))
```

To reduce the multicolinearity effect, we conducted feature selection by removing variables with correlation coefficient > 0.7 and keep the rest 11 variables. After the adjustment, the correlation plot between variables change to:

```{r}
x = read.csv("breast-cancer.csv") %>% 
  mutate(diagnosis = as.numeric(factor(diagnosis)) - 1) %>% 
  select(-id) %>% 
  select(radius_mean, texture_mean, smoothness_mean, concavity_mean, symmetry_mean, fractal_dimension_mean, radius_se, texture_se, smoothness_se, concavity_se, symmetry_se)

corrplot(cor(x), method = "color", tl.col = "blue", tl.srt = 60, tl.cex = 0.7,
         title = "Figure 2: Correlation plot after feature selection", mar=c(0,0,1,0))
```

We standardized the data by the `scale()` function in R, take the first 80% of observations as training dataset, and the rest 20% of observations as testing dataset for model comparison.

## Method

## Logistic Model

Take $Y_i$ as the response of $i_{th}$ observation and follows binary distribution $Bin(\pi_i)$. $\pi_i$ is the probability of $i_{th}$ observation being malignant. By applying the logit link: 
$$g(\mu)=\operatorname{logit}(\mu)=\log \frac{\mu}{1-\mu}$$
we have the logistic regression model:
$$\log \frac{\pi_{i}}{1-\pi_{i}}=X_{i} \beta$$
Thus we have the likelihood function of logistic regression
$$L(\pi)=\prod_{i=1}^{n} f\left(y_{i}\right)=\prod_{i=1}^{n} \pi_{i}^{y_{i}}\left(1-\pi_{i}\right)^{1-y_{i}}$$
$$L(\beta ; X, y)=\prod_{i=1}^{n}\left\{\left(\frac{\exp \left(X_{i} \beta\right)}{1+\exp \left(X_{i} \beta\right)}\right)^{y_{i}}\left(\frac{1}{1+\exp \left(X_{i} \beta\right)}\right)^{1-y_{i}}\right\}$$
Then maximize the log likelihood:
$$l(\beta)=\sum_{i=1}^{n}\left\{y_{i}\left(X_{i} \beta\right)-\log \left(1+\exp \left(X_{i} \beta\right)\right)\right\}$$
By taking derivative with respect to $\beta$, the gradient is:

$$\nabla l(\beta)= \sum_{i=1}^{n}\left(y_{i}-\pi_{i}\right) \boldsymbol{x}_{i} =  X^{T}(Y-\boldsymbol{\pi})$$
where $\pi_{i}=\frac{e^{\beta_{i}}}{1+e^{\beta_{i}}}$

By taking the second derivative, the Hessian matrix can be represented by:

$$
\nabla^{2} l(\beta)=-X^{T} \operatorname{diag}\left(\pi_{i}\left(1-\pi_{i}\right)\right) X
$$
i = 1, ... n. Hessian matrix is negative definite.