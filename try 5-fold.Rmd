---
title: "try 5-fold"
author: "Yijing Tao yt2785"
date: '2022-03-27'
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(glmnet)
library(pdp)
library(caret)
library(modelr)
library(pROC)
library(stats)
```

## data preprocessing

```{r data, message = FALSE }
standardize = function(col) {
  mean = mean(col)
  stdev = sd(col)
  return((col - mean)/stdev)
}

# just standardize the covariates
standardized.data = read.csv(file = "breast-cancer.csv") %>% 
  dplyr::select(radius_mean:fractal_dimension_worst) %>% 
  map_df(.x = ., standardize)

# add back in the response and ids
data = cbind(read.csv(file = "breast-cancer.csv") %>% dplyr::select(diagnosis), standardized.data) %>% 
  mutate(diagnosis = ifelse(diagnosis == "M", 1, 0))

cancer = read.csv("breast-cancer.csv") %>% 
    mutate(diagnosis = factor(diagnosis))
  
y = as.matrix(cancer$diagnosis)
x = scale(as.matrix(cancer[, 2:31]))
```

## lassocd
```{r}
soft_threshold <- function(beta, lambda) {
    sign(beta) * max(abs(beta) - lambda, 0)
}

getP <- function(X, betavec){
    Px <- 1/(1 + exp(-(X %*% betavec)))
    return(Px)
}

getW <- function(Px){
    W <- Px*(1-Px)
    return(Px)
}

getZ <- function(X, y, betavec, Px, W){
    Z <- X %*% betavec + (y - Px)/W
    return(Z)
}


lassoCD <- function(
        X, y, lambda, init_beta, max_iter = 1e4, tol = 1e-8
){
    betavec <- init_beta
    N <- length(y)
    i <- 0
    loss <- 1e5
    prevloss <- Inf
    res <- c(0, loss, betavec)
    cont <- TRUE
    while(i <= max_iter && cont){
        i <- i + 1
        prevloss <- loss
        for(j in 1:length(betavec)){
            Px <- getP(X, betavec)
            W <- getW(Px)
            W <- ifelse(abs(W-0) < 1e-5, 1e-5, W)
            Z <- getZ(X, y, betavec, Px, W)
            betaresj <- betavec
            betaresj[j] <- 0
            Zresj <- X %*% betaresj
            betaj <- 
                soft_threshold(mean(W * X[,j] * (Z - Zresj)), lambda)/mean(W * X[,j] * X[,j])
            betavec[j] <- betaj
            loss <- (1/(2*N))*sum(W * (Z - X %*% betavec)^2) + lambda * sum(abs(betavec))
        }
        #print(loss)
        if(abs(prevloss - loss) < tol || loss < Inf){
            cont <- FALSE
        }
        res <- rbind(res, c(i, loss, betavec))
    }
    return(res)
}

# x1 = data2[1:50, 3:32]
# y1 = data2[1:50, 2]
# lassoCD(as.matrix(x1), y1, 0.5, c(rep(3, 30)), max_iter = 1e4, tol = 1e-8)

coordinatelasso <- function(lambda, dat, s, tol=1e-10, maxiter = 200){
    i <- 0 
    pp <- length(s)
    n <- length(dat$y)
    betavec <- s
    loglik <- 1e6
    res <- c(0, loglik, betavec)
    prevloglik <- Inf # To make sure it iterates 
    while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < inf) {
        i <- i + 1 
        prevloglik <- loglik
        for (j in 1:pp) {
            u <- dat$X %*% betavec
            expu <- exp(u) 
            prob <- expu/(expu+1)
            w <- prob*(1-prob) # weighted
            # avoid coeffcients diverging in order to achieve fitted  probabilities of 0 or 1.
            w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
            z <- u + (dat$y-prob)/w
            # calculate noj
            znoj <- dat$X[,-j] %*% betavec[-j]
            # revise the formula to be z
            betavec[j] <- soft_threshold(mean(w*(dat$X[,j])*(z - znoj)), lambda)/(sum(w*dat$X[,j]*dat$X[,j]))
        }
        loglik <- sum(w*(z-dat$X %*% betavec)^2)/(2*n) + lambda * sum(abs(betavec))
        res <- rbind(res, c(i, loglik, betavec))}  
    return(res)
}

lassoCD.predict <- function(betavec, X_new, y){
    Py <- 1/(1 + exp(-(X_new %*% betavec)))
    Pn <- 1-Py
    res <- list(P_pos = Py, P_neg = Pn, res = Py > Pn)
    res$res = as.numeric(res$res)
    return(res)
}

set.seed(1)
rowTrain <- createDataPartition(y = y,
                                p = 0.7,
                                list = FALSE)
cancer$diagnosis = case_when(cancer$diagnosis == "M" ~ 1,
                             TRUE ~ 0)
trainx <- x[rowTrain,]
trainx = cbind(rep(1,dim(trainx)[1]),trainx)
trainy <- cancer$diagnosis[rowTrain,]
testx <- x[-rowTrain,]
testx = cbind(rep(1,dim(testx)[1]),testx)
testy <- cancer$diagnosis[-rowTrain,]
      fit = lassoCD(trainx, trainy, lambda = 0.5, init_beta = rep(0, 31))
      betas = fit[nrow(fit),2:32]
      pred.cd <- lassoCD.predict(betas, testx, testy)
      pred.obs <- data.frame(pred = pred.cd$res, obs = testy)
      roc <- roc(pred.obs$pred, pred.obs$obs)
      auc <- roc$auc[1]
```

```{r}
lambda.cv = function(lambdas, x, y, k, seed) {
  set.seed(seed)
  data = as.data.frame(cbind(x, y))
  folds = crossv_kfold(data, k = k)
  
  start = rep(0, 31)
  fold.mse <- vector()
  fold.se <- vector()

  for (j in 1:length(lambdas)) {
    fold.errors <- vector()
    for (i in 1:k) {
      trainrow= folds[i,1][[1]][[toString(i)]]$idx
      testrow = folds[i,2][[1]][[toString(i)]]$idx
      
      train.X = x[trainrow,] 
      train.X = scale(as.matrix(cbind(rep(1,dim(train.X)[1]),train.X)))
      test.X = x[testrow,]  
      test.X = scale(as.matrix(cbind(rep(1,dim(test.X)[1]),test.X)))
      
      train.y = y[trainrow,] 
      test.y = y[testrow,] 
      
      # Perform the logistic-LASSO
      fit = lassoCD(train.X, train.y, lambda = lambdas[j], init_beta = start)
      betas = fit[nrow(fit),2:32]
      pred.cd <- lassoCD.predict(betas, test.X, test.y)
      pred.obs <- data.frame(pred = pred.cd$res, obs = cancer$diagnosis[testrow,])
      roc <- roc(pred.obs$pred, pred.obs$obs)
      roc <- roc(test.y, pred)
    }
    start = betas
    auc[j] <- roc$auc[1]
  }
  return(cbind(log.lambda = log(lambdas), auc))
}

lambda.seq = exp(seq(-2,-5, length=100))

cv.path = lambda.cv(lambda.seq, cancer[,3:32], cancer$diagnosis, 5, 2022)
cv.path = as.data.frame(cv.path)
max.auc = max(cv.path$auc)
max.lambda = cv.path[which(cv.path$auc == max.auc),]$log.lambda

#cv.path %>%
#  ggplot(data = ., aes(x = log.lambda, y = auc)) +
#  geom_vline(xintercept = max.lambda) +
#  geom_line(color = "red") +
#  geom_errorbar(aes(ymin = fold.mse - fold.se, ymax = fold.mse + 
#                      fold.se), color = "gray50") +
#  labs(
#   title = "Average Test Fold MSE as a function of lambda",
#   x = "log(lambda)",
#   y = "Average Test MSE"
#   ) +
# theme(plot.title = element_text(hjust = 0.5))

exp(max.lambda)
```
## 5-Fold Cross Validation
The test data is taken from the same data, but is not involved in the training, so that the model can be evaluated relatively objectively to match the data outside the training set. The evaluation of the model in the test data is commonly done by cross-validation. It divides the original data into K groups (K-Fold), and makes each subset of data into a test set separately, and the rest of the K-1 subset data as the training set, so that K models will be obtained. These K models are evaluated separately in the test set, and the final error MSE (Mean Squared Error) is summed and averaged to obtain the cross-validation error.

Cross-validation makes effective use of the limited data and the evaluation results are as close as possible to the performance of the models on the test set, which can be used as an indicator for model optimization.

In order to find the optimal lambda, we use 5-fold cross validation. The dataset is divided into five subdatasets by using the *crossv_kfold* function. We combined 4 of them as the training data set, and the rest $\frac{1}{5}$ of them as the test data set each time. The optimal coefficients is then found five times by running the logit-LASSO on the training data set, leaving a different subset out each time. The subset left out is then used to estimate the model performance. This is done for all lambdas in the pre-defined sequence in order to search for the lambda with the highest average predictive ability. 
We use MSE as the criteria to choose the best tuning parameter and corresponding model obtained from train data, and calculating MSE to evaluate the model performance in test dataset.

After the 5-fold cross validation, we found that the best lambda is about $e^{-3.5152} = 0.0297$ in the optimal model when set.seed(2022). 
