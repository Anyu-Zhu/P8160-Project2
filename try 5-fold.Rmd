---
title: "5-fold cv"
author: "Yijing Tao"
date: "3/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(glmnet)
library(pdp)
library(caret)
library(modelr)
```

## data preprocessing

```{r data, message = FALSE }
standardize = function(col) {
  mean = mean(col)
  stdev = sd(col)
  return((col - mean)/stdev)
}

# just standardize the covariates
standardized.data = read.csv(file = "breast-cancer.csv") %>% 
  dplyr::select(radius_mean:fractal_dimension_worst) %>% 
  map_df(.x = ., standardize)

# add back in the response and ids
data = cbind(read.csv(file = "breast-cancer.csv") %>% dplyr::select(diagnosis), standardized.data) %>% 
  mutate(diagnosis = ifelse(diagnosis == "M", 1, 0))

cancer = read.csv("breast-cancer.csv") %>% 
  janitor::clean_names() %>% 
  mutate(diagnosis = as.numeric(factor(diagnosis)) - 1)
y = as.matrix(cancer$diagnosis)
x = scale(as.matrix(cancer[, 2:31]))

cancer_x = cancer %>% 
  dplyr::select(-id,-diagnosis, -perimeter_mean, -area_mean, -concave_points_mean, -radius_worst, -area_se, -perimeter_worst, -area_worst, -concave_points_worst, -texture_worst, -smoothness_worst, -compactness_se, -compactness_mean, -compactness_worst, -concavity_worst, -fractal_dimension_worst, -perimeter_se, -concave_points_se, -fractal_dimension_se, -symmetry_worst)
x2 = scale(as.matrix(cancer_x))

n = dim(x2)[1]
n_train = floor(n*0.8)

y_train = as.matrix(y[1:n_train,], ncol = 1)
y_test = as.matrix(y[(n_train + 1):n,], ncol = 1)
x_train = x2[1:n_train,]
x_test = x2[(n_train + 1):n,]
```

```{r}
soft_threshold <- function(beta, lambda) {
    sign(beta) * max(abs(beta) - lambda, 0)
}

getP <- function(X, betavec){
    Px <- 1/(1 + exp(-(X %*% betavec)))
    return(Px)
}

getW <- function(Px){
    W <- Px*(1-Px)
    return(Px)
}

getZ <- function(X, y, betavec, Px, W){
    Z <- X %*% betavec + (y - Px)/W
    return(Z)
}

lassoCD <- function(
        X, y, lambda = 0.5, init_beta = NULL, max_iter = 500, tol = 1e-8
){
    betavec <- init_beta
    N <- length(y)
    i <- 0
    loss <- 1e5
    prevloss <- Inf
    res <- c(0, loss, betavec)
    cont <- TRUE
    while(i <= max_iter && cont){
        i <- i + 1
        prevloss <- loss
        for(j in 1:length(betavec)){
            Px <- getP(X, betavec)
            W <- getW(Px)
            W <- ifelse(abs(W-0) < 1e-5, 1e-5, W)
            Z <- getZ(X, y, betavec, Px, W)
            betaresj <- betavec
            betaresj[j] <- 0
            Zresj <- X %*% betaresj
            betaj <- 
                soft_threshold(mean(W * X[,j] * (Z - Zresj)), lambda)/mean(W * X[,j] * X[,j])
            betavec[j] <- betaj
            loss <- (1/(2*N))*sum(W * (Z - X %*% betavec)^2) + lambda * sum(abs(betavec))
        }
        # print(loss)
        if(abs(prevloss - loss) < tol || loss > Inf){
            cont <- FALSE
        }
        res <- rbind(res, c(i, loss, betavec))
    }
    return(res)
}
```


## Cross-Validation
```{r}
lambda.cv = function(lambdas, x, y, k, seed) {
  set.seed(seed)
  data = as.data.frame(cbind(x, y))
  folds = crossv_kfold(data, k = k)
  
  start = rep(0, 12)
  fold.auc <- vector()
  #fold.se <- vector()

  for (j in 1:length(lambdas)) {
    fold.errors <- vector()
    for (i in 1:k) {
      trainrow= folds[i,1][[1]][[toString(i)]]$idx
      testrow = folds[i,2][[1]][[toString(i)]]$idx
      
      train.X = x[trainrow,] 
      train.X = cbind(rep(1,dim(train.X)[1]),train.X)
      test.X = x[testrow,] 
      test.X = cbind(rep(1,dim(test.X)[1]),test.X)
      
      train.y = y[trainrow,] 
      test.y = y[testrow,] 
      
      # Perform the logistic-LASSO
      fit = lassoCD(train.X, train.y, lambda = lambdas[j], init_beta = start)
      betas = fit[nrow(fit),2:13]
      u = test.X %*% betas
      expu = exp(u)
      prob = expu / (1 + expu)
      prob_neg = 1-prob
      pred_y = ifelse(prob>prob_neg,1,0)
      mis_class = sum(pred_y != test.y)
      
      # Calculate the test MSE for the fold
      fold.errors[i] = mean(auc(test.y, prob))
    }
    start = betas
    fold.auc[j] = mean(fold.errors)
    #fold.se[j] = sqrt(var(fold.errors)/k)
  }
  return(cbind(log.lambda = log(lambdas), fold.auc))
}

lambda.seq = exp(seq(-1,-10, length=100))

cv.path = lambda.cv(lambda.seq, x_train, y_train, 5, 11)
cv.path = as.data.frame(cv.path)
max.auc = max(cv.path$fold.auc)
best.lambda = cv.path[which(cv.path$fold.auc == max.auc),]$log.lambda

cv.path %>%
  ggplot(data = ., aes(x = log.lambda, y = fold.auc)) +
  geom_vline(xintercept = best.lambda) +
  geom_line(color = "red") 
  #geom_errorbar(aes(ymin = fold.mse - fold.se, ymax = fold.mse + 
                      #fold.se), color = "gray50") +

exp(best.lambda)

x_train = cbind(rep(1,dim(x_train)[1]),x_train)
bestfit = lassoCD(x_train, y_train, lambda = exp(best.lambda), init_beta = rep(1,12))
```

```{r}
glm(y ~ x2, family = binomial(link = "logit"))$bestTune
```
```{r path_plot}
path <- function(X, y, lambdaseq){
    init_beta <- rep(0, dim(X)[2])
    betas <- NULL
    for (i in 1:length(lambdaseq)) {
        cd.result <- lassoCD(X, y, lambda = lambdaseq[i], init_beta = init_beta)
        last_beta <- cd.result[nrow(cd.result),3:dim(cd.result)[2]]
        init_beta <- last_beta
        betas <- rbind(betas,c(last_beta))
        i <- i + 1
    }
    return(data.frame(cbind(lambdaseq,betas)))
}
path.out <- path(x_train, y_train,lambdaseq = exp(seq(1,-10, length=100)))
colnames(path.out) <- c("lambda","intercept",colnames(x_train)[2:12])
# plot a path of solutions
path.plot <- path.out %>%
  gather(key = par, value = estimate, c(2:dim(path.out)[2])) %>% 
  ggplot(aes(x = log(lambda), y = estimate, group = par, col = par)) +
  geom_line()+
  ggtitle("A path of solutions with a sequence of descending lambda's") +
  xlab("log(Lambda)") + 
  ylab("Estimate") +
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 6))
path.plot
```

```{r}
library(glmnet)
library(caret)

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
glmnGrid <- expand.grid(alpha = 1, lambda = exp(seq(-1,-10, length = 100)))
model.glmn <- train(x = x_train,
                   y = as.factor(ifelse(y_train == 1, "pos", "neg")),
                   method = "glmnet",
                   tuneGrid = glmnGrid,
                   metric = "ROC",
                   trControl = ctrl)
model.glmn$bestTune
glmn.coef = coef(model.glmn$finalModel, model.glmn$bestTune$lambda)
```

