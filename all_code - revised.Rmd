---
title: "All code"
author: "Anyu Zhu"
date: "3/27/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

library(tidyverse)
library(ggplot2)
library(caret)
library(modelr)
library(pROC)
library(glmnet)
```

## Data pre-processing
```{r}
cancer = read.csv("breast-cancer.csv") %>% 
  mutate(diagnosis = as.numeric(factor(diagnosis)) - 1) %>% 
  select(-id)

y = as.matrix(cancer$diagnosis)
x = cancer %>% 
  select(-diagnosis, -perimeter_mean, -area_mean, -concave.points_mean, -radius_worst, 
         -area_se, -perimeter_worst, -area_worst, -concave.points_worst, -texture_worst, 
         -smoothness_worst, -compactness_se, -compactness_mean, -compactness_worst, -concavity_worst, 
         -fractal_dimension_worst, -perimeter_se, -concave.points_se, -fractal_dimension_se, -symmetry_worst) %>% 
  as.matrix() %>% 
  scale()

n = dim(x)[1]
n_train = floor(n*0.8)

y_train = as.matrix(y[1:n_train,], ncol = 1)
y_test = as.matrix(y[(n_train + 1):n,], ncol = 1)
x_train = x[1:n_train,]
x_test = x[(n_train + 1):n,]

x_train_b0 = cbind(rep(1,nrow(x_train)),x_train)
x_test_b0 = cbind(rep(1,nrow(x_test)), x_test)
```

## Logistic
```{r}
logisticstuff <- function(x, y, betavec) {

  x = cbind(1, x)
  colnames(x)[1] = "intercept"
  
  u <- x %*% betavec
  expu <- exp(u)
  # loglikelihood
  loglik <- t(u) %*% y - sum(log(1 + expu))
  p <- expu / (1 + expu)
  # gradient
  grad <- t(x) %*% (y - p)
  # hessian
  Hess <- -t(x) %*% diag(as.vector(p * (1-p))) %*% x

  return(list(loglik = loglik, grad = grad, Hess = Hess))
}

```

## Newton-Raphson

```{r}
NewtonRaphson <- function(x, y, func, start, tol=1e-10, maxiter = 1000) {
  i = 0
  cur = start
  stuff = func(x, y, cur)
  res = c(0, stuff$loglik, cur)
  prevloglik = -Inf # To make sure it iterates
  while(i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    i = i + 1
    prevloglik = stuff$loglik
    prev = cur
    
    # redirection
    hess = stuff$Hess
    hess.eigen = eigen(stuff$Hess)$values
    
    if (sum(hess.eigen) > 0){
      g = max(hess.eigen) + 1
      hess = hess - g*diag(1, dim(hess))
    }
    
    cur = prev - solve(hess) %*% stuff$grad
    
    # step halving
    lambda = 1
    while (func(x, y, cur)$loglik < prevloglik) {
      lambda = lambda / 2
      cur = prev - lambda * solve(hess) %*% stuff$grad
    }
    
    stuff = func(x, y, cur) # log-lik, gradient, Hessian
    res = rbind(res, c(i, stuff$loglik, cur))
    # Add current values to results matrix
  }
  colnames(res) = c("i", "loglik", paste0("beta", 0:(ncol(res)-3)))
  return(res)
}

nr = NewtonRaphson(x_train, y_train, logisticstuff, rep(1,12))

ans = data.frame(nr)
colnames(ans) = c("iteration", "log likelihood")
ggplot(ans, aes(x = iteration, y = `log likelihood`)) +
  geom_point(size = 0.5) +
  geom_line()


```

## lassocd
```{r}
soft_threshold <- function(beta, lambda) {
    sign(beta) * max(abs(beta) - lambda, 0)
}

getP <- function(X, betavec){
    Px <- 1/(1 + exp(-(X %*% betavec)))
    return(Px)
}

getW <- function(Px){
    W <- Px*(1-Px)
    return(Px)
}

getZ <- function(X, y, betavec, Px, W){
    Z <- X %*% betavec + (y - Px)/W
    return(Z)
}


lassoCD <- function(
        X, y, lambda, init_beta, max_iter = 500, tol = 1e-8
){
    betavec <- init_beta
    N <- length(y)
    i <- 0
    loss <- 1e5
    prevloss <- Inf
    res <- c(0, loss, betavec)
    cont <- TRUE
    while(i <= max_iter && cont){
        i <- i + 1
        prevloss <- loss
        for(j in 1:length(betavec)){
            Px <- getP(X, betavec)
            W <- getW(Px)
            W <- ifelse(abs(W-0) < 1e-5, 1e-5, W)
            Z <- getZ(X, y, betavec, Px, W)
            betaresj <- betavec
            betaresj[j] <- 0
            Zresj <- X %*% betaresj
            betaj <- 
                soft_threshold(mean(W * X[,j] * (Z - Zresj)), lambda)/mean(W * X[,j] * X[,j])
            betavec[j] <- betaj
            loss <- (1/(2*N))*sum(W * (Z - X %*% betavec)^2) + lambda * sum(abs(betavec))
        }
        #print(loss)
        if(abs(prevloss - loss) < tol || loss < Inf){
            cont <- FALSE
        }
        res <- rbind(res, c(i, loss, betavec))
    }
    return(res)
}


coordinatelasso <- function(lambda, dat, s, tol=1e-10, maxiter = 200){
    i <- 0 
    pp <- length(s)
    n <- length(dat$y)
    betavec <- s
    loglik <- 1e6
    res <- c(0, loglik, betavec)
    prevloglik <- Inf # To make sure it iterates 
    while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < inf) {
        i <- i + 1 
        prevloglik <- loglik
        for (j in 1:pp) {
            u <- dat$X %*% betavec
            expu <- exp(u) 
            prob <- expu/(expu+1)
            w <- prob*(1-prob) # weighted
            # avoid coeffcients diverging in order to achieve fitted  probabilities of 0 or 1.
            w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
            z <- u + (dat$y-prob)/w
            # calculate noj
            znoj <- dat$X[,-j] %*% betavec[-j]
            # revise the formula to be z
            betavec[j] <- soft_threshold(mean(w*(dat$X[,j])*(z - znoj)), lambda)/(mean(w*dat$X[,j]*dat$X[,j]))
        }
        loglik <- sum(w*(z-dat$X %*% betavec)^2)/(2*n) + lambda * sum(abs(betavec))
        res <- rbind(res, c(i, loglik, betavec))}  
    return(res)
}
```


## mse cv

```{r}
lambda.cv = function(lambdas, x, y, k, seed) {
  set.seed(seed)
  data = as.data.frame(cbind(x, y))
  folds = crossv_kfold(data, k = k)
  
  start = rep(0, ncol(x))
  fold.mse <- vector()
  fold.se <- vector()

  for (j in 1:length(lambdas)) {
    fold.errors <- vector()
    for (i in 1:k) {
      trainrow= folds[i,1][[1]][[toString(i)]]$idx
      testrow = folds[i,2][[1]][[toString(i)]]$idx
      
      train.X = x[trainrow,] 
      test.X = x[testrow,] 
      
      train.y = y[trainrow,] 
      test.y = y[testrow,] 
      
      # Perform the logistic-LASSO
      fit = lassoCD(train.X, train.y, lambda = lambdas[j], init_beta = start)
      betas = fit[nrow(fit),3:ncol(fit)]
      u = test.X %*% betas
      expu = exp(u)
      prob = expu / (1 + expu)
      # Calculate the test MSE for the fold
      fold.errors[i] = mean((test.y - prob)^2)
    }
    start = betas
    fold.mse[j] = mean(fold.errors)
    fold.se[j] = sqrt(var(fold.errors)/k)
  }
  return(cbind(log.lambda = log(lambdas), fold.mse, fold.se))
}

lambda.seq = exp(seq(-2,-12, length=1000))

cv.path = lambda.cv(lambda.seq, x_train_b0, y_train, 5, 2022)
cv.path = as.data.frame(cv.path)
min.mse = min(cv.path$fold.mse)
min.lambda = cv.path[which(cv.path$fold.mse == min.mse),]$log.lambda

cv.path %>%
  ggplot(data = ., aes(x = log.lambda, y = fold.mse)) +
  geom_vline(xintercept = min.lambda) +
  geom_line(color = "red") +
  geom_errorbar(aes(ymin = fold.mse - fold.se, ymax = fold.mse + 
                      fold.se), color = "gray50") +
  labs(
   title = "Average Test Fold MSE as a function of lambda",
   x = "log(lambda)",
   y = "Average Test MSE"
   ) +
 theme(plot.title = element_text(hjust = 0.5))

exp(min.lambda)

min.lambda
```

## auc cv

```{r}
lambda.cv = function(lambdas, x, y, k) {
  data = as.data.frame(cbind(x, y))
  folds = crossv_kfold(data, k = k)
  
  start = rep(0, ncol(x))
  fold.auc <- vector()
  #fold.se <- vector()

  for (j in 1:length(lambdas)) {
    fold.errors <- vector()
    for (i in 1:k) {
      trainrow= folds[i,1][[1]][[toString(i)]]$idx
      testrow = folds[i,2][[1]][[toString(i)]]$idx
      
      train.X = x[trainrow,] 
      test.X = x[testrow,] 
      
      train.y = y[trainrow,] 
      test.y = y[testrow,] 
      
      # Perform the logistic-LASSO
      fit = lassoCD(train.X, train.y, lambda = lambdas[j], init_beta = start)
      betas = fit[nrow(fit),3:ncol(fit)]
      u = test.X %*% betas
      expu = exp(u)
      prob = expu / (1 + expu)
      # Calculate the test MSE for the fold
      fold.errors[i] = mean(auc(test.y, prob))
    }
    start = betas
    fold.auc[j] = mean(fold.errors)
    #fold.se[j] = sqrt(var(fold.errors)/k)
  }
  return(cbind(log.lambda = log(lambdas), fold.auc))
}

lambda.seq = exp(seq(-6,-12, length=500))

cv.path = lambda.cv(lambda.seq, x_train_b0, y_train, 5)
cv.path = as.data.frame(cv.path)
max.auc = max(cv.path$fold.auc)

max.auc

best.lambda = cv.path[which(cv.path$fold.auc == max.auc),]$log.lambda

cv.path %>%
  ggplot(data = ., aes(x = log.lambda, y = fold.auc)) +
  geom_vline(xintercept = best.lambda) +
  geom_line(color = "red") 

exp(best.lambda)

best.lambda
```

## caret

```{r}
library(caret)

ctrl <- trainControl(method = "cv",
                     number = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
glmnGrid <- expand.grid(alpha = 1, lambda = lambda.seq)
model.glmn <- train(x = x_train,
                   y = as.factor(ifelse(y_train == 1, "pos", "neg")),
                   method = "glmnet",
                   tuneGrid = glmnGrid,
                   metric = "ROC",
                   trControl = ctrl)
model.glmn$bestTune
glmn.coef = coef(model.glmn$finalModel, model.glmn$bestTune$lambda)
```



## Path Result
```{r}

```


## Compare with full model
```{r}
lassoCD.predict <- function(betavec, X_new, y){
    Py <- 1/(1 + exp(-(X_new %*% betavec)))
    Pn <- 1-Py
    res <- list(P_pos = Py, P_neg = Pn, res = Py > Pn)
    res$res = as.numeric(res$res)
    return(res)
}

```

## coefficients

```{r}
NR.coef = ans[nrow(ans), 3:ncol(ans)] %>% t()
fit.glm = glm(y_train ~ x_train, family = binomial(link = "logit"))
glm.coef = fit.glm$coefficients %>% as.data.frame()

logistics.coef = cbind(glm.coef, NR.coef)
colnames(logistics.coef) = c("glm package", "Newton-Raphson")

logistics.coef %>% knitr::kable()
```


