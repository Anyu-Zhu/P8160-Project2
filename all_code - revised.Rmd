---
title: "All code"
author: "Anyu Zhu"
date: "3/27/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)


library(tidyverse)
library(ggplot2)
library(caret)
library(modelr)
library(pROC)
library(glmnet)
```

## Data pre-processing
```{r}
cancer = read.csv("breast-cancer.csv") %>% 
  mutate(diagnosis = as.numeric(factor(diagnosis)) - 1) %>% 
  dplyr::select(-id)

y = as.matrix(cancer$diagnosis)
x = cancer %>% 
  dplyr::select(-diagnosis, -perimeter_mean, -area_mean, -concave.points_mean, -radius_worst, 
         -area_se, -perimeter_worst, -area_worst, -concave.points_worst, -texture_worst, 
         -smoothness_worst, -compactness_se, -compactness_mean, -compactness_worst, -concavity_worst, 
         -fractal_dimension_worst, -perimeter_se, -concave.points_se, -fractal_dimension_se, -symmetry_worst) %>% 
  as.matrix()# %>% 
  #scale()

n = dim(x)[1]
n_train = floor(n*0.8)

y_train = as.matrix(y[1:n_train,], ncol = 1)
y_test = as.matrix(y[(n_train + 1):n,], ncol = 1)
x_train = x[1:n_train,]
x_test = x[(n_train + 1):n,]

x_train_b0 = cbind(rep(1,nrow(x_train)),x_train)
x_test_b0 = cbind(rep(1,nrow(x_test)), x_test)
```

## Logistic
```{r}
logisticstuff <- function(x, y, betavec) {

  x = cbind(1, x)
  colnames(x)[1] = "intercept"
  
  u <- x %*% betavec
  expu <- exp(u)
  # loglikelihood
  loglik <- t(u) %*% y - sum(log(1 + expu))
  p <- expu / (1 + expu)
  # gradient
  grad <- t(x) %*% (y - p)
  # hessian
  Hess <- -t(x) %*% diag(as.vector(p * (1-p))) %*% x

  return(list(loglik = loglik, grad = grad, Hess = Hess))
}

```

## Newton-Raphson

```{r}
NewtonRaphson <- function(x, y, func, start, tol=1e-10, maxiter = 1000) {
  i = 0
  cur = start
  stuff = func(x, y, cur)
  res = c(0, stuff$loglik, cur)
  prevloglik = -Inf # To make sure it iterates
  while(i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    i = i + 1
    prevloglik = stuff$loglik
    prev = cur
    
    # redirection
    hess = stuff$Hess
    hess.eigen = eigen(stuff$Hess)$values
    
    if (sum(hess.eigen) > 0){
      g = max(hess.eigen) + 1
      hess = hess - g*diag(1, dim(hess))
    }
    
    cur = prev - solve(hess) %*% stuff$grad
    
    # step halving
    lambda = 1
    while (func(x, y, cur)$loglik < prevloglik) {
      lambda = lambda / 2
      cur = prev - lambda * solve(hess) %*% stuff$grad
    }
    
    stuff = func(x, y, cur) # log-lik, gradient, Hessian
    res = rbind(res, c(i, stuff$loglik, cur))
    # Add current values to results matrix
  }
  colnames(res) = c("i", "loglik", paste0("beta", 0:(ncol(res)-3)))
  return(res)
}

nr = NewtonRaphson(x_train, y_train, logisticstuff, rep(1,12))

ans = data.frame(nr)
colnames(ans) = c("iteration", "log likelihood")
ggplot(ans, aes(x = iteration, y = `log likelihood`)) +
  geom_point(size = 0.5) +
  geom_line()


```

## lassocd
```{r}
soft_threshold <- function(beta, lambda) {
    sign(beta) * max(abs(beta) - lambda, 0)
}

getP <- function(X, betavec){
    Px <- 1/(1 + exp(-(X %*% betavec)))
    return(Px)
}

getW <- function(Px){
    W <- Px*(1-Px)
    return(Px)
}

getZ <- function(X, y, betavec, Px, W){
    Z <- X %*% betavec + (y - Px)/W
    return(Z)
}


lassoCD <- function(
        X, y, lambda, init_beta, max_iter = 500, tol = 1e-8
){
    betavec <- init_beta
    N <- length(y)
    i <- 0
    loss <- 1e5
    prevloss <- Inf
    res <- c(0, loss, betavec)
    cont <- TRUE
    while(i <= max_iter && cont){
        i <- i + 1
        prevloss <- loss
        for(j in 1:length(betavec)){
            Px <- getP(X, betavec)
            W <- getW(Px)
            W <- ifelse(abs(W-0) < 1e-5, 1e-5, W)
            Z <- getZ(X, y, betavec, Px, W)
            Zresj <- X[,-j] %*% betavec[-j]
            betaj <- 
                soft_threshold(mean(W * X[,j] * (Z - Zresj)), lambda)/mean(W * X[,j] * X[,j])
            betavec[j] <- betaj
            loss <- (1/(2*N))*sum(W * (Z - X %*% betavec)^2) + lambda * sum(abs(betavec))
        }
        #print(loss)
        if(abs(prevloss - loss) < tol || loss < Inf){
            cont <- FALSE
        }
        res <- rbind(res, c(i, loss, betavec))
    }
    return(res)
}


```


```{r}
##### 3. coordinate-wise logistic lasso
sfun <- function(beta,lambda) sign(beta) * max(abs(beta)-lambda, 0)
coordinatelasso <- function(lambda, x, y, s, tol=1e-10, maxiter = 200){
  i <- 0 
  pp <- length(s)
  n <- length(y)
  betavec <- s
  loglik <- 1e6
  res <- c(0, loglik, betavec)
  prevloglik <- Inf # To make sure it iterates 
  while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < Inf) {
    i <- i + 1 
    prevloglik <- loglik
    for (j in 1:pp) {
      u <- x %*% betavec
      expu <- exp(u) 
      prob <- expu/(expu+1)
      w <- prob*(1-prob) # weighted
      # avoid coeffcients diverging in order to achieve fitted  probabilities of 0 or 1.
      w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
      z <- u + (y-prob)/w
      # calculate noj
      znoj <- x[,-j] %*% betavec[-j]
      # revise the formula to be z
      betavec[j] <- sfun(mean(w*(x[,j])*(z - znoj)), lambda)/(mean(w*x[,j]*x[,j]))
    }
    loglik <- sum(w*(z-x %*% betavec)^2)/(2*n) + lambda * sum(abs(betavec))
    res <- rbind(res, c(i, loglik, betavec))}  
  return(res)
}
corres <- coordinatelasso(lambda = 0, x_train_b0, y_train, s = rep(0, dim(x_train_b0)[2]) ,maxiter = 2000)

corres[nrow(corres),]
```

## auc cv

```{r}
lambda.cv = function(lambdas, x, y, k) {
  set.seed(2022)
  data = as.data.frame(cbind(x, y))
  folds = crossv_kfold(data, k = k)
  
  start = rep(0, ncol(x))
  fold.auc <- vector()
  #fold.se <- vector()

  for (j in 1:length(lambdas)) {
    fold.errors <- vector()
    for (i in 1:k) {
      trainrow= folds[i,1][[1]][[toString(i)]]$idx
      testrow = folds[i,2][[1]][[toString(i)]]$idx
      
      train.X = x[trainrow,] 
      test.X = x[testrow,] 
      
      train.y = y[trainrow,] 
      test.y = y[testrow,] 
      
      # Perform the logistic-LASSO
      fit = coordinatelasso(lambda = lambdas[j], train.X, train.y, s = start, maxiter = 500)
      betas = fit[nrow(fit),3:ncol(fit)]
      u = test.X %*% betas
      expu = exp(u)
      prob = expu / (1 + expu)
      # Calculate the test MSE for the fold
      fold.errors[i] = mean(auc(test.y, prob))
    }
    start = betas
    fold.auc[j] = mean(fold.errors)
    #fold.se[j] = sqrt(var(fold.errors)/k)
  }
  return(cbind(log.lambda = log(lambdas), fold.auc))
}

lambda.seq = exp(seq(-2,-13, length=500))
 
cv.path = lambda.cv(lambda.seq, x_train_b0, y_train, 5)
cv.path = as.data.frame(cv.path)
max.auc = max(cv.path$fold.auc)

max.auc

best.lambda = cv.path[which(cv.path$fold.auc == max.auc),]$log.lambda

cv.path %>%
  ggplot(data = ., aes(x = log.lambda, y = fold.auc)) +
  geom_vline(xintercept = best.lambda) +
  geom_line(color = "red") 


exp(best.lambda)

best.lambda
```



```{r}
best_beta <- coordinatelasso(lambda = exp(best.lambda), x_train_b0, y_train, s = rep(0,dim(x_train_b0)[2]), maxiter = 500)
```

```{r cv_using_lassoCD}
lambda.cv.lassoCD = function(lambdas, x, y, k) {
  set.seed(2022)
  data = as.data.frame(cbind(x, y))
  folds = crossv_kfold(data, k = k)
  
  start = rep(0, ncol(x))
  fold.auc <- vector()
  #fold.se <- vector()

  for (j in 1:length(lambdas)) {
    fold.errors <- vector()
    for (i in 1:k) {
      trainrow= folds[i,1][[1]][[toString(i)]]$idx
      testrow = folds[i,2][[1]][[toString(i)]]$idx
      
      train.X = x[trainrow,] 
      test.X = x[testrow,] 
      
      train.y = y[trainrow,] 
      test.y = y[testrow,] 
      
      # Perform the logistic-LASSO
      fit = lassoCD(train.X, train.y, lambda = lambdas[j], init_beta = start)
      betas = fit[nrow(fit),3:ncol(fit)]
      u = test.X %*% betas
      expu = exp(u)
      prob = expu / (1 + expu)
      # Calculate the test MSE for the fold
      fold.errors[i] = mean(auc(test.y, prob))
    }
    start = betas
    fold.auc[j] = mean(fold.errors)
    #fold.se[j] = sqrt(var(fold.errors)/k)
  }
  return(cbind(log.lambda = log(lambdas), fold.auc))
}

lambda.seq.lassoCD = exp(seq(-2,-13, length=500))
 
cv.path.lassoCD = lambda.cv.lassoCD(lambda.seq.lassoCD, x_train_b0, y_train, 5)
cv.path.lassoCD = as.data.frame(cv.path.lassoCD)
max.auc.lassoCD = max(cv.path.lassoCD$fold.auc)

max.auc.lassoCD

best.lambda.lassoCD = cv.path.lassoCD[which(cv.path.lassoCD$fold.auc == max.auc.lassoCD),]$log.lambda

cv.path.lassoCD %>%
  ggplot(data = ., aes(x = log.lambda, y = fold.auc)) +
  geom_vline(xintercept = best.lambda.lassoCD) +
  geom_line(color = "red") 


exp(mean(best.lambda.lassoCD))

best.lambda.lassoCD
```
```{r}
best_beta.lassoCD <- lassoCD( x_train_b0, y_train,exp(mean(best.lambda.lassoCD)), init_beta = rep(2,dim(x_train_b0)[2]))
```

## caret

```{r}
library(caret)

set.seed(9543)

ctrl <- trainControl(method = "cv",
                     number = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

glmnGrid <- expand.grid(alpha = 1, lambda = lambda.seq)

model.glmn <- train(x = x_train,
                   y = as.factor(ifelse(y_train == 1, "pos", "neg")),
                   method = "glmnet",
                   tuneGrid = glmnGrid,
                   metric = "ROC",
                   trControl = ctrl)

model.glmn$bestTune


```



## Path Result
```{r}

```




## coefficients

```{r}
# newton rapson
NR.coef = ans[nrow(ans), 3:ncol(ans)] %>% t()

# logistic lasso
NR.lasso.ans = coordinatelasso(lambda = exp(best.lambda), x_train_b0, y_train, s = rep(0, ncol(x_train_b0)), maxiter = 2000)
NR.lasso.coef = NR.lasso.ans[nrow(NR.lasso.ans), 3:ncol(NR.lasso.ans)]

# glm
fit.glm = glm(y_train ~ x_train, family = binomial(link = "logit"))
glm.coef = fit.glm$coefficients %>% as.data.frame()

glmn.coef = coef(model.glmn$finalModel, model.glmn$bestTune$lambda) %>% as.matrix() %>% as.data.frame()

logistics.coef = cbind(glmn.coef, glm.coef, NR.coef, NR.lasso.coef)
colnames(logistics.coef) = c("caret-glmnet", "glm package", "Newton-Raphson", "Logistic-LASSO")

logistics.coef %>% 
  select("glm package", "Newton-Raphson", "Logistic-LASSO", "caret-glmnet") %>% 
  knitr::kable()
```

## Compare with full model
```{r}
lassoCD.predict <- function(betavec, X_new, y){
    Py <- 1/(1 + exp(-(X_new %*% betavec)))
    y.pred = rep(0, nrow(y))
    y.pred[Py > 0.5] = 1
    return(y.pred)
}


```

## test performance

- test

```{r}
y.pred.log.lasso = lassoCD.predict(NR.lasso.coef, x_test_b0, y_test)
y.pred.log = lassoCD.predict(NR.coef, x_test_b0, y_test)
y.pred.glm = lassoCD.predict(as.matrix(glm.coef), x_test_b0, y_test)
y.pred.glmn = lassoCD.predict(as.matrix(NR.lasso.coef), x_test_b0, y_test)
```


```{r, warning=FALSE}
roc.log.lasso <- roc(y_test, y.pred.log.lasso)
roc.log = roc(y_test, y.pred.log)
roc.glmn = roc(y_test, y.pred.glmn)
plot(roc.log.lasso, legacy.axes = TRUE, print.auc = TRUE)
plot(roc.log, legacy.axes = TRUE, print.auc = TRUE)
plot(roc.glmn, legacy.axes = TRUE, print.auc = TRUE)
```

```{r, warning=FALSE}
auc <- c(roc.log$auc[1], roc.log.lasso$auc[1], roc.glmn$auc[1])

modelNames <- c("Newton-Raphson","Newton-Raphson LASSO","glmnet")

ggroc(list(roc.log, roc.log.lasso, roc.glmn), legacy.axes = TRUE, size = 1, alpha = 0.5) + 
  scale_color_discrete(labels = paste0(modelNames, " (", round(auc,3),")"),
                       name = "Models (AUC)") +
  geom_abline(intercept = 0, slope = 1, color = "grey") +
  theme_bw() +
  theme(text = element_text(size = 14))
```

## CV performance

```{r}
lambda.cv.2 = function(lambdas, x, y, k, coefs) {
  set.seed(2022)
  data = as.data.frame(cbind(x, y))
  folds = crossv_kfold(data, k = k)
  
  start = coefs
 
  fold.errors <- vector()
  for (i in 1:k) {
    trainrow= folds[i,1][[1]][[toString(i)]]$idx
    testrow = folds[i,2][[1]][[toString(i)]]$idx
    
    train.X = x[trainrow,] 
    test.X = x[testrow,] 
    
    train.y = y[trainrow,] 
    test.y = y[testrow,] 
    
    # Perform the logistic-LASSO
    fit = coordinatelasso(lambda = lambdas, train.X, train.y, s = start, maxiter = 2000)
    betas = fit[nrow(fit),3:ncol(fit)]
    u = test.X %*% betas
    expu = exp(u)
    prob = expu / (1 + expu)
    # Calculate the test MSE for the fold
    fold.errors[i] = mean(auc(test.y, prob))
  }
  return(fold.errors)
}
```


```{r}
cv.roc.log.lasso = lambda.cv.2(exp(best.lambda), x_train_b0, y_train, k = 5, coefs = NR.lasso.coef)
cv.roc.log = lambda.cv.2(0, x_train_b0, y_train, k = 5, coefs = NR.coef)
cv.roc.glmn = model.glmn$resample$ROC
```


```{r, fig.width=8, fig.height=4}

df = tibble(
  AUC = c(cv.roc.log, cv.roc.log.lasso, cv.roc.glmn),
  model = c(rep("Newton Raphson", 5), rep("Logistic-LASSO", 5), rep("caret-glmnet",5))
)

df %>% 
  ggplot(aes(fill = model)) +
  geom_boxplot(aes(x = AUC, y = model)) +
  scale_fill_manual(values=wes_palettes$GrandBudapest1[c(1, 2, 4)]) +
  theme_bw() +
  theme(text = element_text(size = 14))
```



```{r}
path <- function(X, y, lambdaseq){
    init_beta <- rep(0, dim(X)[2])
    betas <- NULL
    for (i in 1:length(lambdaseq)) {
        cd.result = coordinatelasso(lambda = lambdaseq[i], X, y, s = init_beta, maxiter = 2000)
        last_beta <- cd.result[nrow(cd.result),3:dim(cd.result)[2]]
        init_beta <- last_beta
        betas <- rbind(betas,c(last_beta))
        i <- i + 1
    }
    return(data.frame(cbind(lambdaseq,betas)))
}
path.out <- path(x_train_b0, y_train, lambdaseq = exp(seq(-2,-13, length=100)))
colnames(path.out) <- c("lambda","intercept",colnames(x_train_b0)[2:12])
# plot a path of solutions
path.plot <- path.out %>%
  gather(key = par, value = estimate, c(2:dim(path.out)[2])) %>% 
  ggplot(aes(x = log(lambda), y = estimate, group = par, col = par)) +
  geom_line()+
  ggtitle("A path of solutions with a sequence of descending lambda's") +
  xlab("log(Lambda)") + 
  ylab("Estimate") +
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 6))
path.plot + theme_bw()
```


